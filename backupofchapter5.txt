%===================================== CHAP 5 =================================
\section{One dimensional limited domain examples to highlight convergence problems and robustness}
Since $L(\matr{X}$ is not convex, the initial estimate is important and any iteration is doomed to fall into a local minimum.
Some of these local minima occur at estimates that are worse than others, as we shall see.
Here we discuss some types of convergence problems and how to deal with them, in particular how to initialize the estimates for \matr{F} and \matr{X}. 

To evaluate the robustness and some problems with the convergence of the algorithm, we turn to a simple example of simulated data where a one-dimensional variable is controlling the response of the neurons, whose tuning is defined as Gaussian bumps placed randomly along the domain of \matr{X}. 
The domain of \vect{x} is defined as $\vect{x} \in [0,10] \subset \R^1$. 
An example of a real world situation with this geometry is a box for a rodent that is narrow, where the neurons are place cells positioned along the corridor (is it ok to assume Gaussian bumps for place cells? Find an article about place cells to cite.)
The ''path'' of \vect{x} is sampled from a generative Gaussian process identical to the prior of \matr{X}, but with the option of changing the hyperparameters. 
These hyperparameters influence the smoothness and scaling of the generated path, and it is desirable to have a path that stretches across the entire domain of \vect{x} in order to infer the tuning correctly for each neuron. 

To make sure that the simulated path stays within the alloted domain, one alteration is added to the Gaussian process prior: Whenever the simulated path strays outside the domain, it is folded back by mirroring the path about the limit. 
Also, in the case that the path does not cover the entire interval, we scale it up so that it fits the entire interval before spikes are generated.

To make sure that the path of \matr{X} moves wide enough to cover the entire domain, the variance parameter $\sigma_x$ in the Gaussian process prior generating \matr{X} must be adjusted. 

For this example the background tuning noise is set rather low, to -5, corresponding to each neuron having an expected number of spikes equal to 0.006 when \matr{X} is infinitely far away from the neuron's tuning curve's peak, and an expected number of spikes equal to 


Area for tuning: 180 neurons, 100 with peaks in range of X. Number per meter decided by total range of X. 

Figure \ref{example} shows some simulated tuning curves and an example path in the domain.
\begin{figure}[H]
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        %\includegraphics[width=0.98\linewidth]{fig/52-tuning.png}
        \includegraphics[width=0.98\linewidth]{fig/2020-06-14-paral-robust-true-f.png}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{fig/53-path.png}
    \end{minipage}
    \caption{Tuning curves and an example of a generated path for the latent variable, kept between the limits of 0 and 10 by folding it back whenever the path goes outside the domain.}
    \label{example}
\end{figure}

\subsection{Handling of flipped estimates}
Note that this is not an exhaustive list of the problems local minima may lead to. %The values of \matr{F} and \matr{X} may for example converge to local minima where the estimate of \matr{X} has minSince the log posterior of \matr{X} is not convex, the starting point of X may determine how good the final estimate is.
%We know that a quadratic form of a symmetric matrix is maximized by the eigenvector corresponding to the largest eigenvalue of the symmetric matrix representing the quadratic form \citep{hardle2007appliedp63}. However, here we optimize over $K_x$, and the maximizer is not obvious. 
First, observe that $L(\matr{X})$ is an even function: 
\begin{equation}
\begin{aligned}
    L(-\matr{X}) 
    &= - {\frac{N}{2}} \log |K_{(-x)}| -\frac{1}{2} \sum_{i=1}^N
    \Big( \vect{f}_i^TK_{(-x)}^{-1}\vect{f}_i \Big) -\frac{1}{2} \sum_{j=1}^P \Big( (-\vect{x}_j^T)K_t^{-1}(-\vect{x}_j) \Big)
    \Bigg] \\
    &= - {\frac{N}{2}} \log |K_x| -\frac{1}{2} \sum_{i=1}^N
    \Big( \vect{f}_i^TK_x^{-1}\vect{f}_i \Big) -\frac{1}{2} \sum_{j=1}^P \Big( \vect{x}_j^TK_t^{-1}\vect{x}_j \Big)
    \Bigg] \\
    &= L(\matr{X})
\end{aligned}
\end{equation}
$K_{(-x)} = K_x$ because the squared exponential covariance function only depends on \matr{X} through the squared distance $(\vect{x}_{t_i} - \vect{x}_{t_j})^2$.
%Second, the xprior term has no connection to the tuning curves and only acts as a scaling and smoothness prior for \matr{X}. In fact, it is maximized by the zero vector since $K_t^{-1}$ is a symmetric positive semidefinite matrix and therefore only has positive eigenvalues.
Since $L(\matr{X})$ is even, any local minimum $\hat{\matr{X}}$ will be repeated for $-\hat{\matr{X}}$. 
Because the offset of the estimate cannot be trusted either (see next section), what we observe is that the algorithm is just as likely to converge to an estimate that is upside down as not (with the correct mean value, or offset, but upside down.) 

Figure \ref{upsidedown1} shows how different random initial positions may lead to estimates that are either upside down or correctly aligned. 
This is actually not a problem in cases where the true \matr{X} is known, since the estimate of \matr{X} can be rotated after convergence if it is upside down. (If true X is not known, then no problem too because the domain don't matter?)

\begin{figure}[H]
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{fig/2020-06-07-ii-initial-stopped.png}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{fig/2020-06-07-ii-final-stopped.png}
    \end{minipage}
    \caption{Left: 7 Initial random initial estimates, where the value at every time point $\vect{x}_t$ is sampled independently from a uniform distribution in the range from 0 to $2\pi$. The value $2\pi$ is arbitrary and does not imply periodicity. Right: Final estimates. }
    \label{upsidedown1}
\end{figure}
Figures \ref{flip1} through \ref{flip3} show how an iteration first converges upside down, 
Initial convergence
\begin{figure}[H]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/06-2020-06-07-ii-T-1000-lambda-8-seed-0.png}
    \caption{Convergence for a starting position that is uniform random between 0 and $2\pi$, plotted with one color per iteration step. }
    \label{k1}
\end{figure}

Convergence after flipping
\begin{figure}[H]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/06-2020-06-07-ii-T-1000-lambda-8-seed-0-flipped.png}
    \caption{Convergence for a starting position that is uniform random between 0 and $2\pi$, plotted with one color per iteration step. }
    \label{k1}
\end{figure}

Final estimate
\begin{figure}[H]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/06-2020-06-07-ii-T-1000-lambda-8-seed-0-final.png}
    \caption{Convergence for a starting position that is uniform random between 0 and $2\pi$, plotted with one color per iteration step. }
    \label{k1}
\end{figure}

The posterior of \matr{F} estimate is based on the inference of \matr{X}, and will the estimated tuning curves will therefore be mirrored if \matr{X} is upside down.

\subsection{Partly flipped estimates}
Instead of being entirely flipped, \matr{X} may fall into a local minima where it is partly upside down, which is more problematic since it cannot be handled bu just flipping it. 

In figure \ref{upsidedown2}, the number of initial estimates has been increased from 7 to 20, and estimates that ended up upside down have been flipped if their $L(\matr{X})$ value improved when the estimate was flipped (Flipping X based on L value: best L value also has best RMSE score it seems!!! We should not be able to check this from L values!!!!!!). This solves the problem of entirely flipped estimats. 
But one of the random initial starts has converged to an estimate that appears to be partly upside down (blue line). From an entirely random intial position, it has become trapped in a local minimum from which it did not escape. 

\begin{figure}[H]
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{fig/2020-06-07-ii-initial.png}
        %\label{kt1}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{fig/2020-06-07-ii-final.png}
    \end{minipage}
    \caption{Left: 20 Initial starting points for the \matr{X} estimate. Right: Final estimates corrected for flipping. }
    \label{upsidedown2}
\end{figure}

The bigger $T$ is, the bigger this problem is, because there are more places where the flipping can occur (OR: if we can solve it, good.)
%Figure \ref{k1} shows an example with $T=1000$ and a uniform random initial estimate for \matr{X}.
%\begin{figure}[H]
%    \centering
%    \includegraphics[width=0.98\linewidth]{fig/04-2.png} %fig/2020-06-06-ii-T-1000-lambda-8-seed-0.png}%{fig/01.png}
%    \caption{Convergence for a starting position that is uniform random between 0 and $2\pi$, plotted with one color per iteration step. }
%    \label{k1}
%\end{figure}
%It appears as if the flipping is correct for higher \matr{X} values than about 4, and flipped whenever the X values are lower. 

%One iteration from an entirely flat start at $\vect{x}=1$: 
%\begin{figure}[H]
%    \centering
%    \includegraphics[width=0.98\linewidth]{fig/06.png}
%    \caption{Convergence for a starting position that is uniform random between 0 and $2\pi$, plotted with one color per iteration step. }
%    \label{k1}
%\end{figure}

%Flat start around $\vect{x}=1$, with random noise: 
%\begin{figure}[H]
%    \centering
%    \includegraphics[width=0.98\linewidth]{fig/08.png}
%    \caption{Convergence for a starting position that is uniform random between 0 and $2\pi$, plotted with one color per iteration step. }
%    \label{k1}
%\end{figure}

%Start at path and true F values for comparison: 
%RMSE for X: 0.2890430891684393 compared to 0.17342200792078588 for flat start!!
%\begin{figure}[H]
%    \centering
%    \includegraphics[width=0.98\linewidth]{fig/09-true-start.png}
%    \caption{} %tolerance = 1e-5
%    \label{k1}
%\end{figure}

\subsection{Consequences of flipped estimates for tuning curves}
-- This is interesting.
If a path is consistently upside down everywhere, the F estimates should be shifted so that instead of having a bump for low value, we have it for high value. 
If however the path is partly upside down, we would then have tuning curves with two equal bumps. 
%If we know that the neurons are selective, this can be limited by imposing some penalty for having more high values than average?

-- Plot tuning curves for X estimate that is normal, upside down and partly upside down --

\subsection{Finding the right offset for \matr{X}}
Just offset, similar to no offset:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/2020-06-12-paral-T-1000-lambda-4-seed-11-final.png}
    \caption{}
    \label{k1}
\end{figure}
Offset and scaling together in one function: 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/offsetscale-2020-06-13-paral-robust-T-1000-lambda-4-seed-11.png}
    \caption{}
    \label{k1}
\end{figure}
Scaling and offset separately as two functions: 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/scale-and-offset-separately-2020-06-13-paral-robust-T-1000-lambda-4-seed-11.png}
    \caption{}
    \label{k1}
\end{figure}
But we don't want to do optimization over this: We can just compare to the true X...
Then what happens to the inducing pionts? 
After scaling and offsetting X estimate according to true X, the inducing points are set in this range as well. Okay. 



--------

Translation invariant. Having the grid points for \matr{X} is not necessarily enough!

In the original $L(\matr{X}$ function, before the introduction of inducing points, the only term that is related to the values in \matr{F} is the quadratic term, and the covariance matrix $K_x$ is unchanged under translations of \matr{X}: $K(\matr{X}) = K(\matr{X} + c)$. 
This means that there is no assurance that the final estimate of \matr{X} will have the correct offset. 

However, with the introduction of inducing points, a fixed grid in the domain of \matr{X} is introduced. 
What happens now? 

Maybe we will show that if the range of the grid is aligned with the min and max estimates of the true \matr{X}, then the algorithm is able to infer the offset of \matr{X} correctly?
Figure \ref{offset} shows two convergence plots. The grid of inducing points is evenly spaced in both examples, but in the first one the range is between the minimum and maximum value of the true path, while for the second, the range is from 0 to $2\pi$.

\begin{figure}[H]
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{2020-06-08-paral-robust-T-100-lambda-8-seed-11.png}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{2020-06-08-paral-robust-T-100-lambda-8-seed-11 (1).png}
    \end{minipage}
    \caption{}
    \label{offset}
\end{figure}

If the grid is placed outside of the actual range of the true \matr{X} path, then the correct shape can still be inferred: 
\begin{figure}[H]
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{fig/gridbetweenpi2pi.png}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{fig/gridbetweenpi2pi.png}
    \end{minipage}
    \caption{}
    \label{offset}
\end{figure}

\subsection{Scaling}
Analytically, the $L(\matr{X})$ function should distinguish between different scaled versions of \matr{X}.
A scaled-up estimate of \matr{X} means that the off-diagonal values in $K_x$will on average be lower, since the \vect{x} values are further apart. 
This is however no guarantee that the algorithm will converge to the right scaling for \matr{X}. 
Indeed, since the \matr{F} values are only firmly positioned in time and not space, any kind of scaling goes? No! If the assumed tunings in space are too sharp, that is unlikely under the \matr{F} prior. But there is in general no problem under the \matr{F} smoothness prior to have too wide tuning curves. 
The inducing points are key to defining the scale I think. 
Damn dude!! Since wider tuning curves are more smooth, are they not more probably under the \matr{F} smoothness prior???? Then we would be biased towards scaling up \matr{X} as much as possible, but are in practice limited by the range of the inducing points!!!
A simple test with wide inducing grid would confirm this. 

In tests we see that scaling is not right. 
Scaled \matr{X} estimate gives scaled positions of \matr{F}:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/2020-06-12-paral-robust-T-1000-lambda-4-seed-11.png}
    \caption{}
    \label{k1}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{fig/2020-06-12-paral-robust-tuning-47.png}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{fig/2020-06-12-paral-robust-tuning-76.png}
    \end{minipage}
    \caption{}
    \label{example}
\end{figure}


\subsection{A comment on the placement of the inducing grid}
Good positioning of the inducing grid requires knowledge of the domain and range of \matr{X}. 
For the illustrative example above, the inducing points were placed in a grid with uniform spacing in the range of the exact domain. 
If the range of \vect{x} is not known, one approach is to start with a wide range for the inducing points and then set the range equal to the range of the estimate of \matr{X} at every iteration as it changes.

Good placement gives immediate convergence:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/2020-06-12-paral-robust-T-1000-lambda-4-seed-11.png}
    \caption{}
    \label{k1}
\end{figure}
Too wide grid gives entirely different result:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/2020-06-13-paral-robust-T-1000-lambda-4-seed-11.png}
    \caption{Good and to wide placement of inducing grid. First is spot on, second too wide.}
    \label{k1}
\end{figure}

Grid is placed too widely across the range vs placed spot on and kept spot on:
\begin{figure}[H]
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{fig/40-PCA-doingitsjobwhenthereisactivityacrossthewholerange.png}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{fig/41-indgridtoowide.png}
    \end{minipage}
    \caption{Good and to wide placement of inducing grid. First is spot on (with PCA), second too wide.}
    \label{inducingplacement}
\end{figure}

\subsection{Conclusion and commentary about convergence and segue to initial positions for F and X}
Discussion of ensemble starting: Drawback: Uses resources that could be spent adding timesteps.
PCA plus sampled "noise" from the Xprior? Interesting!! 

\section{Initialization}
%\subsection{Importance of initial position}
\subsection{Initial estimate for \matr{F}}
Since the log posterior of \matr{F} is a convex funtion, the initial estimate of \matr{F} would not be important if the true \matr{X} is known. But the initial \matr{F} greatly influences the resulting \matr{X} estimate, and thus in turn the \matr{F} estimate that depends on \matr{X}.
Therefore, if we start with a bad estimate for \matr{F} we may end up in a local minimum for \matr{X}, which will in turn affect the estimate of \matr{F}.
To find a good starting pont for \matr{F}, the matrix of spikes, \matr{Y}, can be used. 

With a Poisson likelihood model, $f_{i,t} = \log \lambda_{i,t} = \log E[y_{i,t}]$. Based on this, a sensible initial \matr{F} would be 
\begin{equation}
\begin{aligned}
    \matr{F}_{\text{initial}} = \log{\matr{Y} + \epsilon}
\end{aligned}
\end{equation}
where some $\epsilon < 0$ is introduced to take the logarithm when $y_{i,t} = 0$. 

However, some trial and error showed that the following initialization gave better results:
\begin{equation}
\begin{aligned}
    \matr{F}_{\text{initial}} = \sqrt{\matr{Y}} - \frac{\text{max}(\sqrt{\matr{Y}})}{2} 
\end{aligned}
\end{equation}
Figures \ref{ftrue}, \ref{finit} show true f, spikes and two inits as color maps of the matrix values.
\begin{figure}[H]
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{fig/49-true-f.png}
        %\label{kt1}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{fig/49-sqrt-initial.png}
    \end{minipage}
    \caption{Shows true f values and initial X with sqrt. }
    \label{ftrue}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{fig/49-spikes.png}
        %\label{kt1}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{fig/49-variant-initial.png}
    \end{minipage}
    \caption{Shows two different initializations.}
    \label{finit}
\end{figure}

To make these values more understandable, it is useful to find the posterior mean on a uniform grid of points in the domain of \matr{X}, where we assume that the true \matr{X} path is known. 
For T=1000, baseline tuning of -5, tuning strength of 4, this path (seed 0): %details in goodplot-11-06
\begin{figure}[H]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/2020-06-11-paral-robust-T-1000-path.png}
    \caption{This is the wrong }
    \label{roobplot}
\end{figure}
Show sqrt initial tuning curves provided true path: better in centre than in edges. 
\begin{figure}[H]
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{fig/66-2020-06-12-paral-robust-tuning-0.png}
        %\label{kt1}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{fig/67-sqrt-start-2020-06-12-paral-robust-tuning-50.png}
    \end{minipage}
    \caption{sqrt()}
    \label{diff_ts}
\end{figure}
This also plays a part in the inference of \matr{X} since there are less neurons to guide it there. 
The log intialization of F:
\begin{figure}[H]
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{fig/68-2020-06-12-paral-robust-tuning-0.png}
        %\label{kt1}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{fig/69-log-start-2020-06-12-paral-robust-tuning-50.png}
    \end{minipage}
    \caption{log}
    \label{diff_ts}
\end{figure}
Exactly what this does with the shape of the $L(\matr{X})$ function is not straightforward to say, but it is intuitive that larger values in \matr{F} makes the quadratic term more important relative to the logdet and xprior terms. This certainly impacts the estimate of \matr{X} for the first iteration.

Note that in order to find a good posterior estimate of \matr{F}, it is essential that the \matr{X} estimate is good. To illustrate this, we provide the initial estimate for \matr{F} as described above, but instead of using the true path to find the posterior of the tuning curves, we use an \matr{X} estimate where the \vect{x} value has been sampled uniformly in the range of \matr{X}. Figure \ref{shitstart} shows that the posterior is useless in this case, and we see clearly that the estimate of \matr{F} depends on the estimate of \matr{X}.
\begin{figure}[H]
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{fig/70-2020-06-12-paral-robust-tuning-0.png}
        %\label{kt1}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{fig/70-2020-06-12-paral-robust-tuning-50.png}
    \end{minipage}
    \caption{Shit initial}
    \label{shitstart}
\end{figure}

For the Bernoulli likelihood model, the most obvious option is to set 
\begin{equation}
\begin{aligned}
    \matr{F}_{\text{initial}} = \matr{Y}.
\end{aligned}
\end{equation}

\subsubsection{First iteration: Start by finding X given F}
At the first iteration, a choice must be made between first finding \matr{F} | \matr{X} and then \matr{X} | \matr{F}, or doing it in the opposite order. 
Since a decent estimate of \matr{F} is readily available from the spike matrix \matr{Y}, 
Since the initial \matr{F} estimate is pretty good.

based on observed spikes, it is a darn good estimate. 

\subsection{Initial estimate for \matr{X}}
Different initial estimates of \matr{X} should be considered due to the $L(\matr{X})$ function not being convex.
One option is to use an ensemble of starting points and take the best result. Other options like swarm optimization (ref if used before, otherwise put in further work idea: Use swarm optimization and find way to disentangle partial flipping by just using L function or looking for jumps in X) exist.

In addition to this, the initial estimate can be chosen as either a path drawn from a gaussian process prior with some hyperparameters, can be random noise, a flat line, or one can apply another dimensionality reduction tool, for example PCA, on the observed spikes \matr{Y} to find an initial estimate of \matr{X}.
To attempt this, first the observed spike matrix is smoothed with a Gaussian filter independently per neuron, and then PCA was applied. 
A smoothing length of 15 time bins was chosen after visual inspection to make the PCA align with the true \matr{X}.
For a one-dimensional variable the first principal component is chosen as the initial \matr{X} estimate. A smoothing length must be chosen. This was but we would expect it to be similar in size to the length scale parameter $\sigma_x$ of the Gaussian process prior of \matr{X}.

An example PCA initialization is shown together with the true path of \matr{X} in figure \ref{pcaexample}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/2020-06-14-paral-robust-T-1000-PCA-initial.png}
    \caption{}
    \label{pcaexample}
\end{figure}
Visibly, it seems like the first principal component has captured some of the shape of the true \matr{X}, and would be a good initial point. 
But surprisingly the PCA initialization leads to a worse estimate than a uniformly flat start where all the \vect{x} values are set to 1, as is shown in figures \ref{pcabad} and \ref{flatgood}.
\begin{figure}[H]
    \centering
    %\includegraphics[width=0.98\linewidth]{fig/2020-06-14-paral-robust-T-1000-lambda-4-seed-11-pca-bad.png}
    \includegraphics[width=0.98\linewidth]{fig/2020-06-14-paral-T-1000-lambda-4-seed-11-final-pca-bad.png}
    \caption{PCA bad. }
    \label{pcabad}
\end{figure}

\begin{figure}[H]
    \centering
    %\includegraphics[width=0.98\linewidth]{fig/2020-06-14-paral-robust-T-1000-lambda-4-seed-11-flatgood.png}
    \includegraphics[width=0.98\linewidth]{fig/2020-06-14-paral-T-1000-lambda-4-seed-11-finalflat-good.png}
    \caption{Flat good}
    \label{flatgood}
\end{figure}

To show how the estimates of the tuning curves are affected by the estimate of \matr{X}, figures ---- --- -show the posterior means. 
\begin{figure}[H]
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{fig/2020-06-14-paral-robust-tuning-0-pca-bad.png}
        %\label{kt1}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{fig/2020-06-14-paral-robust-tuning-50-pca-bad.png}
    \end{minipage}
    \caption{PCA bad.}
    \label{diff_ts}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{fig/2020-06-14-paral-robust-tuning-0flat-good.png}
        %\label{kt1}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{fig/2020-06-14-paral-robust-tuning-50flat-good.png}
    \end{minipage}
    \caption{Flat good.}
    \label{diff_ts}
\end{figure}

This is unexpected!
Then, proof of the opposite when we add more noise...
No golden truth here. 

If however, we increase the background noise level to an expected number of 2 spikes per time bin, and also increase the expected number of spikes at the top of the tuning curve from 4 to 6, then the PCA initialization is better. 

\begin{figure}[H]
    \centering
    %\includegraphics[width=0.98\linewidth]{fig/2020-06-14-paral-robust-T-1000-lambda-4-seed-11-pca-bad.png}
    \includegraphics[width=0.98\linewidth]{fig/2020-06-14-paral-T-1000-lambda-6-seed-11-final-pca.png}
    \caption{PCA better for higher noise and tuning. }
    \label{pcabetter}
\end{figure}

\begin{figure}[H]
    \centering
    %\includegraphics[width=0.98\linewidth]{fig/2020-06-14-paral-robust-T-1000-lambda-4-seed-11-flatgood.png}
    \includegraphics[width=0.98\linewidth]{fig/2020-06-14-paral-T-1000-lambda-6-seed-11-final.png}
    \caption{Flat worse for higher noise and tuning. }
    \label{flatworse}
\end{figure}


Should we add smoothing-regularization?

Is the precense of some background noise helpful for inference? 

\subsection{Trivia and case study of bad initializations}
Show that it difficult to find X if it is a straight line?

\subsection{Examples of different initial X}
Starting given true path and true spike responses (ground truth ''optimal'' solution):
\begin{figure}[H]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/09-true-start.png}
    \caption{Given true path but not F everything (this plot is not given true F!!!!!.}
    \label{roobplot}
\end{figure}

Given only true F but not path:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/09-true-Fnotpath.png}
    \caption{Given true path but not F everything (this plot is not given true F!!!!!.}
    \label{roobplot}
\end{figure}

Given only true path but not F
\begin{figure}[H]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/09-true-pathnotF.png}
    \caption{Given true path but not F everything (this plot is not given true F!!!!!.}
    \label{roobplot}
\end{figure}

-- other initialization examples--

What more can we do? 
Ensemble of starting points like global optimization.
Genetic algorithm that checks Likelihood value of local sections for ensemble solvings and takes final estimate as combination of parts (bonus bonus bonus mission).

\section{Inference of \matr{F}}
This may or may not be less interesting than I thought: Since we run through both the inducing grid and the plotgrid, whose range are set respectively to min and max of inducing and min and max of inducing and of f tuning, the only consequence of bad \matr{X} estimate comes through $K_xg$, formerly $K_uf$, which I now have made reliant on an \matr{X} estimate. This can get good: 
Show what happens to the estimate of \matr{F} if the \matr{X} estimate is bad: 

Maybe these are useful:


These show given entirely random X:
\matr{X}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/57-2020-06-11-paral-T-1000-lambda-8-seed-11-final.png}
    \caption{This is the wrong }
    \label{roobplot}
\end{figure}
\matr{F}:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/58-2020-06-11-simulated-em-tuning-50.png}
    \caption{F.}
    \label{roobplot}
\end{figure}

These show given PCA start
\matr{X}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/57-2020-06-11-paral-T-1000-lambda-8-seed-11-final.png}
    \caption{This is the wrong }
    \label{roobplot}
\end{figure}
\matr{F}:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/58-2020-06-11-simulated-em-tuning-50.png}
    \caption{F.}
    \label{roobplot}
\end{figure}

Not upside down: 
\matr{X}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/57-2020-06-11-paral-T-1000-lambda-8-seed-11-final.png}
    \caption{This is the wrong }
    \label{roobplot}
\end{figure}
\matr{F}:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/58-2020-06-11-simulated-em-tuning-50.png}
    \caption{F.}
    \label{roobplot}
\end{figure}

Upside down: 
\matr{X}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/57-2020-06-11-paral-T-1000-lambda-8-seed-11-final.png}
    \caption{This is the wrong }
    \label{roobplot}
\end{figure}
\matr{F}:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/58-2020-06-11-simulated-em-tuning-50.png}
    \caption{F.}
    \label{roobplot}
\end{figure}



\section{Robustness evaluation}
Two important factors that influence the inference power of the algorithm is how strongly the neurons are tuned to the latent variable, and how many time bins have been observed. 
One would expect that more time bins means more data and thus a better fit of the model overall. 
Also, a stronger tuning should help the inference, since the difference between actual response to a latent variable is distinguishable from noisy spiking. 

A tuning strength will always be relative to a background firing rate for values of \vect{x} that a neuron is not tuned to. We will measure tunig strength as expected number of spikes per time bin at the peak of the Gaussian tuning curve. The background noise level will be the limit value where the distance between \matr{x} and the peak goes towards infinity.
In this example we will first set the background firing rate to $4.5\cdot 10^{-5}$.

Figure \ref{diff_ts} shows how the estimates of \matr{X} vary with the tuning strength of the neurons in  a setting where everything else is kept the same. 
-- examples of different fits for diffreent tuning strengths to show that it's a thing. --
-- check different background noise levels too --
The loss function we will use to describe how well the inference works is root mean squared error (RMSE) between the estimated \matr{X} and the true Ã¥ath of \matr{X}. 

A disclaimer: these rmse values could probably be lower for some other configuration of hyperparameters and optimization settings. The key purpose is to look at the point around which increasing the tuning strength of the neurons does not improve the inference. (Though of course,  this may in turn also depend on the hyperparameters, but at least this is *something*.)

Plot for baseline noise = 1. lam\_array = [1.01,1.1,1.5,2,3,4,5,6,7,8]

Plot for baseline noise gone or = 2

\begin{figure}[H]
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{fig/skip-F-infirstestimate.png}
        %\label{kt1}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{fig/startbyinferringFbasedonshittyXandshowthehorror.png}
    \end{minipage}
    \caption{Left: 20 Initial starting points for the \matr{X} estimate. Right: Final estimates corrected for flipping. }
    \label{diff_ts}
\end{figure}

The number of time bins and the tuning strength relative to the background firing rate of the neurons are two parameters that greatly influence the performance of the algorithm. 
To explore how the root mean squared error relates to the number of time bins, we construct a set of 5 test paths for each number of time bins $T = (10,100,1000)$. 
We choose an array of tuning strengths, and for each tuning strength and choice of T we let the algorithm converge on the five test cases, and we find the root mean square error between the estimate and the true path, which we average over the 5 test cases.
Figure \ref{robplot} shows the average RMSE values for each combination of $T$ and tuning strength. 

\begin{figure}[H]
    \centering
%    \includegraphics[width=0.98\linewidth]{fig/10.png}
    \includegraphics[width=0.98\linewidth]{fig/2020-06-15-plot-robustness.png}
    \caption{Robustness plot with 95 \% confidence intervals. Mean from 20 individual random paths for each T value.}
    \label{roobplot}
\end{figure}
\begin{figure}[H]
    \centering
%    \includegraphics[width=0.98\linewidth]{fig/10.png}
    \includegraphics[width=0.98\linewidth]{different-backgroundnoise.png}
    \caption{}
    \label{roobplot-2}
\end{figure}

Try to explain why the RMSE is bigger for higher T values.

Importance of tuning strength and number of time bins.
4 seeds for initial start, take best L value. Average performance over 5 different paths generated from the Gaussian process prior of \matr{X}. 

Another factor is the level of background noise. It is not obvious whether a higher tuning strength (relatively) is required if the background noise is bigger, or if it scales uniformly. 
-- comparison of rmse values for same offset tuning strengths with different background noise levels --

\section{Inactive neurons are bad}

A comment on the range of tuning: 
Ideally, we want to be able to infer \matr{X} equally well at all points of it domain, and would therefore want to have equally many neurons responding to \matr{X} at every point.
Consider then the edges of the domain, where neurons are active whose peak lies outside the domain (see figure right). 
These neurons will generally have significantly fewer spikes than the rest.
Unfortunately, observations have shown that adding neurons that have few spikes in total has a negative impact on the inference of \matr{X}.
Therefore, we are faced with a tradeoff where we must forgo some ''neural density'' at the edges of the domain in order to not add too many inactive neurons. 
In these test cases, the number of neurons with peaks inside the dmain of \matr{x} was set to 100, and there were 10 neurons on each side with peaks outside the domain. 
This gives us a total of $N = 120$ neurons. The true tuning curves are shown in 


Near the edges of the domain this means that proves challenging, because 
This means that near the edges we would prefer to have neurons 
have the same ''neural density'' everywhere on the domain of \matr{x}, we would like to have 
Observations showed that the 

Should we comment that it is bad to have inactive neurons in the recording?
When I add inactive neurons on the sides things are worse!!!!


\section{Observation about how the three different terms contribute} 
Here we will make some remarks on the effects of the different terms in $L(\matr{X})$ in the convergence.
The fit seems to be better with only quadratic part of f prior term. At least without the normalizing logdet term..
This may also just be from using the wrong hyperparameters

\section{Periodic covariance kernel}
Initialization using PCA for periodic data: Either use 1D, or use PCA with 2 dimensions, then take angle(t) as the head deirection we're interested in. Only using our knowledge about the problem.


\section{Application to head direction data by Peyrache}
Naive approach: Disregard continuity at the edges, just wrap around
Then periodic kernel and remember $\text{x\_grid\_induce}$.

\section{Difference between Poisson and Bernoulli performance}

\subsection{Confidence intervals etc}
For standard errors, bootstrap the data?

\subsubsection{Parameters that we choose: Bin width, tuning width, tuning strength, tuning background noise, number of neurons, number of timebins. }
Appendix food for nerds: Values of all these for examples shown. 

Hyperparameters:
''... found by an exhaustive manual search for a minimum over hyperparameters,'' - Understanding

\section{Optional}
Let sigma n drop all the way

Add Lasso penalty for good measure

To see if we can toss logdet term, plot its values for a range of X estimates, from flat to heaviside; or from flat to linspace

\section{To do:}

\begin{itemize}
    \item Argue why upside down estimates are as good as regular!?
\end{itemize}
